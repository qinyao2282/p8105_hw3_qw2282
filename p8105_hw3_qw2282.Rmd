---
title: "p8105_hw3_qw2282"
author: "Qinyao Wu"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Instal the data
devtools::install_github("p8105/p8105.datasets")
devtools::install_github("thomasp85/patchwork")

library(p8105.datasets)
library(ggridges)
library(tidyverse)
library(patchwork)

#Make the output of figure in the same size. 
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

#Set the theme. 
theme_set(theme_bw() + theme(legend.position = "right"))
```

```{r}
data(brfss_smart2010)

brfss_smart2010 = janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% #Focus on the overall health topic. 
  mutate(response = forcats::fct_relevel(response, c("Excellent","Very good" , "Good", "Fair", "Poor")))

#state observed in seven locations
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr, locationdesc) %>%
  summarize(n = n()) %>% 
  group_by(locationabbr) %>% 
  summarize(n = n()) %>% 
  filter(n == 7) %>% 
  knitr::kable()
#States are CT, FL, NC
```

In 2002, CT, FL and NC are the states that observed at seven locations. This indicates that a lot of responses were observed in these three states complared to the others. 

```{r}
num_observations = brfss_smart2010 %>% 
  group_by(year, locationabbr) %>% 
  summarize(n = n())

#Make the sphaghetti plot
ggplot(data = num_observations, aes(x = year, y = n, color = locationabbr)) + geom_line() + 
  labs(
    title = "observations in Each State",
    x = "Years",
    y = "Number of Observations"
  )

```

The sphaghetti plot is shown with colored locations. From the plot, we can observe that most of the states have observations between zero and a hundred. And most of the states does not have large changes from 2002 to 2010. However, there are some outliers, such as FL, which has extremely high number of observations in 2007 and 2010. This might influence the mean and standard deviation. 


```{r}

#Generate the summarize data for 2002, 2006, 2010
summary_ny_excellent = brfss_smart2010 %>% 
  group_by(year) %>% 
  filter(locationabbr == "NY" & (year == 2002 | year == 2006 | year == 2010)) %>% 
  select(-c(topic, class, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  summarize(mean_excellent = mean(Excellent),
            sd_excellent = sd(Excellent))
  
  knitr::kable(summary_ny_excellent, digits = 1)
  

```





```{r}
#calculate the mean proportion of each response at the state level in each year.
mean_response = brfss_smart2010 %>% 
  group_by(locationabbr, year, response) %>% 
  select(-c(topic, class, question, sample_size, confidence_limit_low:geo_location)) %>% 

  #Calculate the mean. 
  summarize(average_response = mean(data_value, na.rm = TRUE)) 


#Make the violin plot 
ggplot(mean_response, aes(x = year, y = average_response)) +
  geom_violin(aes(fill = factor(year)),  alpha = .5) + 
  
  #Split the panel with response
  facet_grid(~response) + 
  stat_summary(fun.y = median, geom = "point", color = "blue", size = 1) + 
  viridis::scale_fill_viridis(discrete = TRUE) + 
  
  #Add the labels. 
  labs(
    title = "Distribution of Proportion of Response",
    x = "Year",
    y = "State Level Average of proportion"
  )

```

From the violin plot, we can observe that there are very little changes in the proportion of responses throughout the years from 2002 to 2010. The proportion for "Very Good" is always the highest while the proportion for "Poor" is always the lowest. The median has very slight changes that is hard to observe. 

##Problem 2

```{r}
#Read in the data
data(instacart)

```


This data set contains `r nrow(instacart)` and `r ncol(instacart)`.  Here is a list of the variables in this data set `r colnames(instacart)`. This table is used to record the orders received throughout a week. The item names and the aisles, in addtion to the order information are also included. For example, the first line of the data represents the Bulgarian Yogurt, on aisle of Yougut in department of Dairy Eggs, is order on the fourth day of the week. Some key variables are order_number, order_dow, product_name, and aisle. This variables are important because this can help people to decide which day is important for getting certain product, and where this product can be found. 


```{r}
instacart %>% 
  group_by(aisle) %>% #134 aisles
  summarize(n = n()) %>% 
  mutate(order_ranking = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 2) %>%  #Fresh Vegetables
  knitr::kable()

```



```{r}


instacart %>%
  group_by(aisle_id) %>% 
  summarize(n = n()) %>% 
  ggplot( aes(x = aisle_id, y = n)) + 
  geom_point() + 
  labs(
    title = "Aisle Oder number distribution plot",
    x = "Aisle ID",
    y = "Number of orders placed"
  )


```

This scatter plot shows the total number of order in each aisle. From the plot we can observe that there are several outliers in the plot,\ which indicates this aisles have extremely high order numbers. And the other ones has much fewer orders.


```{r}

#Find the most popular item. 

instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  count() %>%
  group_by(aisle) %>% 
  filter(min_rank(desc(n)) < 2) %>% #Get the highest number and the corresponding product name. 
  knitr::kable()


#Find the average order time in each day of the week. 
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%  #Filter to get the products 
  group_by(product_name, order_dow) %>%
  
  #calculate the mean hour. 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
   spread(key = order_dow, value = mean_hour) %>% #Spread to make a table
  knitr::kable(digits = 1)
  
  
  
```



##Problem 3

```{r}
#Read the data set. 
data(ny_noaa) 

#Do some cleaning to the dataset. 
ny_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("Year", "Month", "Day"), sep = "-") %>% #Separate the date
  janitor::clean_names() %>% 
  rename(snow_mm = snow, tmax_0.1_celcius = tmax, tmin_0.1_celcius = tmin, prcp_0.1_mm = prcp, snwd_mm = snwd) #Rename some columns to add the appropriate units. 


ny_data %>% 
  group_by(snow_mm) %>%
  summarise(n = n()) %>% 
  mutate(temp_ranking = min_rank(desc(n))) %>% #Rank the numer of mm of snow. 
  filter(min_rank(desc(n)) < 2)  #0 is the most frequently observed value. 
```  

The most commonly observed value for snow fall is zero. This is because most of the time throughout the year, new york do not get any snow. As a result, a snow fall value of 0 mm will be observed most frequently. 
  

```{R}  
ny_jan_jul = ny_data %>% 
  group_by(id, month) %>% 
  mutate(tmax_0.1_celcius = as.numeric(tmax_0.1_celcius, na.rm = TRUE)) %>% 
  filter(month == "01" | month == "07") %>%
  summarize(mean_temp = mean(tmax_0.1_celcius)) %>% 
  na.omit(mean_temp) 

  ggplot(ny_jan_jul, aes( x = id, y = mean_temp)) +
  geom_point() + 
  facet_grid(~month) + 
  labs(
    title = "Average Temperature in Jan and Jul Across the Year",
    x = "Stations",
    y = "Temperature/ 0.1 Celcius"
  )
 
  ggplot(ny_jan_jul, aes(x = factor(month), y = mean_temp)) +
    geom_boxplot() +
    labs(
    title = "Average Temperature in Jan and Jul Across the Year",
    x = "Months",
    y = "Temperature/ 0.1 Celcius"
  )
 

```

The observable structure of the pattern is that the temperature in all stations in January is near or under zero degrees, while the temperarture in all stations in july is over 20 degrees. There is no obvious outliers that can be observed in the scatter plot. However, with the addtional boxpplot, the outliers can be seen more clearly. The temperature in July has two outliers, one above 30 degree Celcius and one below 25 degree Celcius. 


```{r}
library(patchwork)

figure_temp = ny_data %>% 
  na.omit(tmin_0.1_celcius) %>% 
  na.omit(tmax_0.1_celcius) %>% 
  
  #Change the numbers in temperature into numerics, remove the NA. 
  mutate(tmax_0.1_celcius = as.numeric(tmax_0.1_celcius, na.rm = TRUE)) %>%
  mutate(tmin_0.1_celcius = as.numeric(tmin_0.1_celcius, na.rm = TRUE)) %>%
  
  #Make the plot
  ggplot(aes(x = tmin_0.1_celcius, y = tmax_0.1_celcius)) + 
  geom_hex() +
  
  theme(legend.position = "bottom") +
  labs(
    title = "Tmax and Tmin for NY",
    x = "Tmin/ 0.1 Celcius",
    y = "Tmax/ 0.1 Celcius"
  )

#Plot a figure with snow between 0 and 100 mm. 
figure_snow = ny_snow = ny_data %>% 
  filter(snow_mm > 0, snow_mm < 100)  %>% 
  ggplot(aes(x = snow_mm, color = year)) +
  geom_density(alpha = 0.2) +
  theme(legend.position = "bottom") +
  labs(
    title = "Snow Distribution bwtween 0 and 100 for NY",
    x = "Snow/mm"
  )

#Combine the two figures together. 
figure_temp + figure_snow
```

From these hex plots, we first notice that the temprature in the middle are more commonly observed. The lighter blue indicates the temperature combination is very common. And from the snow distribution figure, we can observe that the most commonly observed snow is from 0 to 25. And snow above 100 mm is very rare.  




