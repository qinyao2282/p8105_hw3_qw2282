---
title: "p8105_hw3_qw2282"
author: "Qinyao Wu"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)

library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r}
data(brfss_smart2010)

brfss_smart2010 = janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% #Focus on the overall health topic. 
  mutate(response = forcats::fct_relevel(response, c("Excellent","Very good" , "Good", "Fair", "Poor")))

#state observed in seven locations
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr, locationdesc) %>%
  summarize(n = n()) %>% 
  group_by(locationabbr) %>% 
  summarize(n = n()) %>% 
  filter(n == 7)  
#States are CT, FL, NC

num_observations = brfss_smart2010 %>% 
  group_by(year, locationabbr) %>% 
  summarize(n = n())

#Make the sphaghetti plot
ggplot(data = num_observations, aes(x = year, y = n, group = locationabbr)) + geom_line()

summary_ny_excellent = brfss_smart2010 %>% 
  group_by(locationabbr) %>% 
  filter(locationabbr == "NY" & (year == 2002 | year == 2006 | year == 2010)) %>% 
  select(-c(topic, class, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(Excellent),
            sd_tmax = sd(Excellent))

summary_ny_excellent

#################################################
ã€€b = brfss_smart2010 %>% 
  group_by(year, locationabbr) %>% 
  select(-c(topic, class, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 

  summarize(mean_excellent = mean(Excellent),
            mean_very_good = mean(`Very good`),
            mean_good = mean(Good),
            mean_fair = mean(Fair),
            mean_poor = mean(Poor)) %>% 
   gather(key = mean_value, value = mean_proportion, mean_excellent:mean_poor)


 ggplot(b, aes(x = mean_proportion, fill = mean_value)) +
  geom_density(alpha = .5) + 
  facet_grid(~year) + 
  viridis::scale_fill_viridis(discrete = TRUE)



```



##Problem 2

```{r}
data(instacart)


```

This data set contains `r nrow(instacart)` and `r ncol(instacart)`.  Here is a list of the variables in this data set `r colnames(instacart)`


```{r}
instacart %>% 
  group_by(aisle) %>% #134 aisles
  summarize(n = n()) %>% 
  mutate(order_ranking = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 2)  #Fresh Vegetables
```

```{r}
instacart %>%
  ggplot( aes(x = aisle_id)) + 
  geom_histogram() + 
  labs(
    title = "Aisle distribution plot",
    x = "Aisle ID",
    y = "Number of orders placed"
  )
```

```{r}
instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  count() %>%
  group_by(aisle) %>% 
  summarize(most_popular_item =  product_name[n == max(n)])


a = instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour)
  
  
  
```



##Problem 3

```{r}
data(ny_noaa) 

ny_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("Year", "Month", "Day"), sep = "-") %>% 
  janitor::clean_names() 


ny_data %>%  
   group_by(snow) %>%
  summarise(n= n()) %>% 
  mutate(temp_ranking = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 2)  #0 is the most frequently observed value. 
  
  
  
  
ny_jan_jul = ny_data %>% 
  group_by(month) %>% 
  filter(month == 11 | month == 1) %>%

  summarize(mean_temp = tmax, na.rm = TRUE)
  
  

```

